# -*- coding: utf-8 -*-
"""MultiClass classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PpOiqjWytV2HmoNWNHflMXZ6mX7tt0dC
"""

!pip install pyforest
from pyforest import *
from math import sqrt, fabs, exp
import matplotlib.pyplot as plot
from sklearn import ensemble
from sklearn.metrics import roc_auc_score, roc_curve
import numpy

target_url = ("https://archive.ics.uci.edu/ml/machine-learning-"
"databases/glass/glass.data")
df = pd.read_csv(target_url, header=None)
df.columns = ['Id', 'RI', 'Na', 'Mg', 'Al', 'Si','K', 'Ca', 'Ba', 'Fe', 'type']
df.set_index('Id', inplace=True)
print('Data loading:')
df.head()

print('Checking details in the dataset:')
d = numpy.array(['building_windows_float_processed', 'building_windows_non_float_processed',
                 'vehicle_windows_float_processed','containers',  'tableware', 'headlamps'])

print("Keys:", df.keys()) # print keys of dataset

# shape of data and target
print("Data shape", df.shape) 
print("Target shape", df.type.shape) # (150,)

print("data:", df[:4]) # first 4 elements

# unique targets
print("Unique targets:", np.unique(df['type'])) 
# counts of each target
print("Bin counts for targets:", np.bincount(df['type']))

print("Feature names:", df.drop(columns= ['type'], axis=1).columns)
print("Target names:", d)

"""Attribute Information:

1. Id number: 1 to 214
2. RI: refractive index
3. Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)
4. Mg: Magnesium
5. Al: Aluminum
6. Si: Silicon
7. K: Potassium
8. Ca: Calcium
9. Ba: Barium
10. Fe: Iron
11. Type of glass: (class attribute)
-- 1 building_windows_float_processed
-- 2 building_windows_non_float_processed
-- 3 vehicle_windows_float_processed
-- 4 vehicle_windows_non_float_processed (none in this database)
-- 5 containers
-- 6 tableware
-- 7 headlamps


The glass problem presents chemical compositions of various
types of glass. The objective of the problem is to determine the use for the glass.

The possible types of glass include glass from building windows, glass from
vehicle windows, glass containers, and so on. The motivation for determining
the type of glass is forensics. At the scene of an accident or a crime, there are fragments of glass, and determining their origin can help determine who is at fault or who committed the crime.
"""

df.describe().transpose()

plt.figure(figsize=(10,5))
boxplot = X_norm.boxplot(column= ['RI', 'Na', 'Mg', 'Al', 'Si',
'K', 'Ca', 'Ba', 'Fe'])
plt.xlabel("Attribute Index")
plt.ylabel(("Quartile Ranges"))
plt.show()

df['type'].value_counts()

"""The box plot of the glass data attributes shows quite a few outliers.
The glass data have a couple of elements that may drive the outlier behavior. One is that the problem is a classification problem. There’s not necessarily any continuity in relationship between attribute values and class membership—no reason to expect proximity of attribute values across classes. Another unique feature of the glass data is that it is somewhat unbalanced. The number of examples of each class runs from 76 for the most populous class to 9 for the least populous.

The average statistics can be dominated by the values for the most populous
classes and there’s no reason to expect members of other classes to have similar
attribute values. The radical behavior can be a good thing for distinguishing
classes from one another, but it also means that a method for making predictions
has to be able to trace a fairly complicated boundary between the different
classes.
"""

plt.figure(figsize=(10,5))

pd.plotting.parallel_coordinates(
    df[['RI', 'Na', 'Mg', 'Al', 'Si','K', 'Ca', 'Ba', 'Fe', 'type']], 
    'type')
plt.show()

plt.figure(figsize=(10,4))
X = df[['RI', 'Na', 'Mg', 'Al', 'Si','K', 'Ca', 'Ba', 'Fe']]
X_norm = (X - X.mean())/X.std()
X_norm = pd.concat([X_norm, df['type']], axis=1)
pd.plotting.parallel_coordinates(X_norm, 'type')
plt.title("Parallel coordinate plot by glass type")
plt.show()

!pip install ppscore
import ppscore as pps

sns.heatmap(pps.matrix(df[['type', 'RI', 'Na', 'Mg', 'Al', 'Si',
'K', 'Ca', 'Ba', 'Fe']]),annot=True,fmt=".2f")
print('Predictive power score')
plt.show()

#Creating the dependent variable class
factor = pd.factorize(df['target'])
df.target = factor[0]
definitions = factor[1]
print(df.target.head())
print(definitions)

d = numpy.array(['building_windows_float_processed', 'building_windows_non_float_processed',
                           'vehicle_windows_float_processed','containers',  'tableware', 'headlamps'])
d

from collections import Counter

# summarize the shape of the dataset
print(df.shape)
# summarize the class distribution
target = df.values[:,-1]
counter = Counter(df['type'])
for k,v in counter.items():
	per = v / len(df) * 100
	print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))

import pandas.testing as tm
plt.style.use('ggplot')
plt.figure(figsize=(10,5))
figure, ax = plt.subplots(1,1, figsize=(10,5))

#variable_names = numpy.array(['building_windows_float_processed', 'building_windows_non_float_processed',
#                            'vehicle_windows_float_processed','containers',  'tableware', 'headlamps'])

sns.countplot(x = 'type', data=df)
ax.set_xticklabels( ('building_windows_float_processed', 'building_windows_non_float_processed',
                             'vehicle_windows_float_processed','containers',  'tableware', 'headlamps'), rotation = 90 )
plt.show()

target_url = ("https://archive.ics.uci.edu/ml/machine-learning-"
"databases/glass/glass.data")
df = pd.read_csv(target_url, header=None)
df.columns = ['Id', 'RI', 'Na', 'Mg', 'Al', 'Si','K', 'Ca', 'Ba', 'Fe', 'type']
df.set_index('Id', inplace=True)

X = df.drop(columns = ['type'], axis=1)
y = df['type']

#from skmultilearn.model_selection import iterative_train_test_split
#X_train, y_train, X_test, y_test = iterative_train_test_split(X, y, test_size = 0.20)
X_train, X_test, y_train, y_test = train_test_split(X, y,
                                               test_size=0.20, stratify=y, random_state=123)

from sklearn.ensemble import RandomForestClassifier

miss_class_error = []
nTreeList = range(50, 2000, 50)
for iTrees in nTreeList:
  depth = None
  max_feat = 4 #try tweaking
  rfc = RandomForestClassifier(n_estimators=iTrees,
                                             max_depth=depth, max_features=max_feat,
                                             oob_score=True,n_jobs=-1, random_state=123)
print('RandomForest model fit:')
rfc.fit(X_train, y_train)

from sklearn.model_selection import learning_curve
plt.style.use('ggplot')
plt.figure(figsize=(10,5))

# Create CV training and test scores for various training set sizes
train_sizes, train_scores, test_scores = learning_curve(
                                                        RandomForestClassifier(),
                                                        X, y, cv=5,
                                                        # Performance metric
                                                        scoring='accuracy',
                                                        # Use all computer cores
                                                        n_jobs=-1,
                                                        # Sizes of 50
                                                        # training set
                                                        train_sizes=np.linspace(0.01,1.0, 100))

# Create means and standard deviations of training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
# Create means and standard deviations of test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Draw lines
plt.plot(train_sizes, train_mean, '--', label="Training score")
plt.plot(train_sizes, test_mean, color="#111111", label="Cross-validation score")
# Draw bands
plt.fill_between(train_sizes, train_mean - train_std,train_mean + train_std)
plt.fill_between(train_sizes, test_mean - test_std,test_mean + test_std)

# plot
plt.title("RandomForest Classifier Learning Curve")
plt.xlabel("Training Set Size"), plt.ylabel("Accuracy Score"),
plt.legend(loc="best"), plt.tight_layout()
plt.show()

from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve

"""### Measuring the Accuracy"""

prediction = rfc.predict(X_test)
score = accuracy_score(y_test, prediction)

print('Accuracy for a single decision stump:', round(score,2))
print("Miss classification Error:" , round((1-score),2))

from sklearn.metrics import confusion_matrix

pd.crosstab(y_test, prediction, rownames=['Actual'], colnames=['Predicted'], margins=True)

#generate confusion matrix
cm = confusion_matrix(y_test, prediction)
pd.DataFrame(cm)

plt.figure(figsize=(10,6))
print('')
print("Confusion Matrix plot")
sns.heatmap(cm.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');

from sklearn.multiclass import OneVsRestClassifier
from sklearn.metrics import roc_curve, auc
from cycler import cycler
from sklearn.preprocessing import label_binarize

y = label_binarize(y, classes=[1,2,3,5,6,7])
n_classes = 6
xtrain, xtest, ytrain, ytest = train_test_split(X, y, stratify=y,
                                                test_size=0.20, random_state=123)
clf = OneVsRestClassifier(RandomForestClassifier(random_state=123))
yscore = clf.fit(xtrain, ytrain).predict_proba(xtest)

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(ytest[:, i], yscore[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute micro-average ROC curve and ROC area
fpr["micro"], tpr["micro"], _ = roc_curve(ytest.ravel(), yscore.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

plt.figure()
lw = 2
plt.plot(fpr[2], tpr[2], color='darkorange',
         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])
plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic example')
plt.legend(loc="lower right")
plt.show()

"""### ROC curves for the multiclass"""

from numpy import interp
from itertools import cycle


# First aggregate all false positive rates
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
# Then interpolate all ROC curves at this points
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])
# Finally average it and compute AUC
mean_tpr /= n_classes

fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure(figsize=(10,6))
plt.plot(fpr["micro"], tpr["micro"],label='micro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["micro"]),color='deeppink', linestyle=':', linewidth=4)

plt.plot(fpr["macro"], tpr["macro"],label='macro-average ROC curve (area = {0:0.2f})'
               ''.format(roc_auc["macro"]),color='navy', linestyle=':', linewidth=4)

colors = cycle(['aqua', 'darkorange', 'cornflowerblue'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,label='ROC curve of class {0} (area = {1:0.2f})'
             ''.format(i, roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=lw)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic to multi-class')
plt.legend(loc="lower right")
plt.show()

from sklearn.metrics import precision_recall_curve, roc_curve

yprob = rfc.predict_proba(xtest)

macro_roc_auc_ovo = roc_auc_score(ytest, yprob, multi_class="ovo",average="macro")
weighted_roc_auc_ovo = roc_auc_score(ytest, yprob, multi_class="ovo",average="weighted")
macro_roc_auc_ovr = roc_auc_score(ytest, yprob, multi_class="ovr",average="macro")
weighted_roc_auc_ovr = roc_auc_score(ytest, yprob, multi_class="ovr",average="weighted")
print("One-vs-One ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
      "(weighted by prevalence)"
      .format(macro_roc_auc_ovo, weighted_roc_auc_ovo))
print("One-vs-Rest ROC AUC scores:\n{:.6f} (macro),\n{:.6f} "
      "(weighted by prevalence)"
      .format(macro_roc_auc_ovr, weighted_roc_auc_ovr))

from sklearn import metrics
print(metrics.classification_report(prediction, y_test))

#plot training and test errors vs number of trees in ensemble
plt.plot(nTreeList, missCLassError)
plt.xlabel('Number of Trees in Ensemble')
plt.ylabel('Missclassification Error Rate')
#plot.ylim([0.0, 1.1*max(mseOob)])
plt.show()

plt.figure(figsize=(10,6))
#variable_names = numpy.array(['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca',
#'Ba', 'Fe', 'Type'])

# Plot feature importance
feature_importance = rfc.feature_importances_

# normalize by max importance
feature_importance = feature_importance / feature_importance.max()

#plot variable importance
idxSorted = numpy.argsort(feature_importance)
barPos = numpy.arange(idxSorted.shape[0]) + .5
plt.barh(barPos, feature_importance[idxSorted], align='center')
plt.yticks(barPos, variable_names[idxSorted])
plt.xlabel('Variable Importance')
plt.show()

from sklearn.model_selection import validation_curve
plt.figure(figsize=(14,6))
# Create range of values for parameter
param_range = np.arange(1, 250, 2)

# Calculate accuracy on training and test set using range of parameter values
train_scores, test_scores = validation_curve(RandomForestClassifier(), X, y, 
                                             param_name="n_estimators", param_range=param_range,
                                             cv=5, scoring="accuracy",n_jobs=-1)

# Calculate mean and standard deviation for training set scores
train_mean = np.mean(train_scores, axis=1)
train_std = np.std(train_scores, axis=1)
# Calculate mean and standard deviation for test set scores
test_mean = np.mean(test_scores, axis=1)
test_std = np.std(test_scores, axis=1)
# Plot mean accuracy scores for training and test sets
plt.plot(param_range, train_mean, label="Training score")
plt.plot(param_range, test_mean, color="b", label="Cross-validation score")
# Plot accurancy bands for training and test sets
plt.fill_between(param_range, train_mean - train_std,train_mean + train_std)
plt.fill_between(param_range, test_mean - test_std,test_mean + test_std)
plt.title("Validation Curve With Random Forest")
plt.xlabel("Number Of Trees")
plt.ylabel("Accuracy Score")
plt.tight_layout()
plt.legend(loc="best")
plt.show()

from xgboost import XGBClassifier

#instantiate model
nEst = 500
depth = 3
learnRate = 0.003
maxFeatures = 3
subSamp = 0.5
xgb = XGBClassifier(n_estimators=nEst,objective='multi:softmax',num_class = 6,
                                 max_depth=depth,learning_rate=learnRate,
                                 max_features=maxFeatures,subsample=subSamp)

#train
xgb.fit(xTrain, yTrain)

print('Accuracy for Gradient Booing: {}'.format(xgb.score(xTest, yTest)))

y_pred = xgb.predict(xTest) 
pred = [round(value) for value in y_pred]
accuracy = accuracy_score(yTest, pred) 

print("Accuracy for Gradient Booing: %.2f%%" % (accuracy * 100.0))

from sklearn.metrics import classification_report, confusion_matrix

cm = confusion_matrix(yTest, pred)
print("Confusion Matrix:")
print(cm)

print("Classification Report")
print(classification_report(yTest, pred))

# calculate prediction
precision = precision_score(yTest, pred, labels=[1,2,3,5,6,7], average='micro')
print('Precision: %.3f' % precision)

from sklearn.metrics import recall_score
# calculate recall
recall = recall_score(yTest, pred, labels=[1,2,3,5,6,7], average='micro')
print('Recall: %.3f' % recall)

plt.figure(figsize=(10,6))

# Plot feature importance
featureImportance =xgb.feature_importances_
# normalize by max importance
featureImportance = featureImportance / featureImportance.max()
#plot variable importance
idxSorted = numpy.argsort(featureImportance)
barPos = numpy.arange(idxSorted.shape[0]) + .5
plot.barh(barPos, featureImportance[idxSorted], align='center')
plot.yticks(barPos, glassNames[idxSorted])
plot.xlabel('Variable Importance')
plot.show()

from sklearn.metrics import precision_score
print(precision_score(yTest, best_preds, average='macro'))

#generate confusion matrix
pList = prediction.tolist()
confusion_matrix = confusion_matrix(yTest, pList)
print('')
print("Confusion Matrix")
print(confusion_matrix)

from sklearn.metrics import confusion_matrix
plt.figure(figsize=(10,6))

print('')
print("Confusion Matrix plot")
mat = confusion_matrix(yTest, prediction)
sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False)
plt.xlabel('true label')
plt.ylabel('predicted label');